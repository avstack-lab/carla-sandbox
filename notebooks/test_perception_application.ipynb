{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1f286-505f-46c2-9d4f-a2578bd4b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avapi\n",
    "import avstack\n",
    "import pandas\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f69b8c-23ee-48ce-ab33-03d45249bed5",
   "metadata": {},
   "source": [
    "## Define Perception/Tracking Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb079f7b-2057-4fa3-bc5b-381d26e76ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _AgentModel():\n",
    "    \"\"\"Base class for agent fusion algorithms\"\"\"\n",
    "    def __init__(self, perception, tracking):\n",
    "        self.perception = perception\n",
    "        self.tracking = tracking\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.ingest(*args, **kwargs)\n",
    "        \n",
    "    def ingest(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# -----------------------------\n",
    "# PERCEPTION ONLY\n",
    "# -----------------------------\n",
    "\n",
    "class EgoCameraPerception(_AgentModel):\n",
    "    \"\"\"Only performs ego-based perception\"\"\"\n",
    "    def __init__(self, perception, *args, **kwargs):\n",
    "        super().__init__(perception, None)\n",
    "\n",
    "    def ingest(self, ego_image, agents_images):\n",
    "        return self.perception['ego']['camera'](ego_image)\n",
    "\n",
    "\n",
    "class AgentsCameraPerception(_AgentModel):\n",
    "    \"\"\"Only performs agent-based perception\"\"\"\n",
    "    def __init__(self, perception, *args, **kwargs):\n",
    "        super().__init__(perception, None)\n",
    "\n",
    "    def ingest(self, ego_image, agents_images):\n",
    "        return [self.perception['agent']['camera'](image) for image in agents_images]\n",
    "\n",
    "\n",
    "class EgoLidarPerception(_AgentModel):\n",
    "    \"\"\"Only performs ego-based perception\"\"\"\n",
    "    def __init__(self, perception, *args, **kwargs):\n",
    "        super().__init__(perception, None)\n",
    "\n",
    "    def ingest(self, ego_pc, agents_pcs):\n",
    "        return self.perception['ego']['lidar'](ego_pc)\n",
    "\n",
    "\n",
    "class AgentsLidarPerception(_AgentModel):\n",
    "    \"\"\"Only performs ego-based perception\"\"\"\n",
    "    def __init__(self, perception, *args, **kwargs):\n",
    "        super().__init__(perception, None)\n",
    "\n",
    "    def ingest(self, ego_pc, agents_pcs):\n",
    "        return [self.perception['agent']['lidar'](pc) for pc in agents_pcs]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PERCEPTION AND TRACKING\n",
    "# -----------------------------\n",
    "\n",
    "class CameraPerceptionAndTracking(_AgentModel):\n",
    "    \"\"\"Only performs ego-based perception and tracking\"\"\"\n",
    "    def __init__(self, perception, *args, **kwargs):\n",
    "        tracking = avstack.modules.tracking.tracker2d.BasicBoxTracker2D()\n",
    "        super().__init__(perception, tracking)\n",
    "\n",
    "    def ingest(self, ego_image, agents_images):\n",
    "        tracks_ego = self.tracking(self.perception['ego']['camera'](ego_image))\n",
    "        return tracks_ego\n",
    "\n",
    "        \n",
    "class LidarPerceptionAndTracking(_AgentModel):\n",
    "    \"\"\"Only performs ego-based perception and tracking\"\"\"\n",
    "    def __init__(self, perception, *args, **kwargs):\n",
    "        tracking = avstack.modules.tracking.tracker3d.BasicBoxTracker3D()\n",
    "        super().__init__(perception, tracking)\n",
    "\n",
    "    def ingest(self, ego_pc, agents_pcs):\n",
    "        tracks_ego = self.tracking(self.perception['ego']['lidar'](ego_pc))\n",
    "        return tracks_ego\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PERCEPTION AND TRACKING WITH FUSION\n",
    "# -----------------------------\n",
    "\n",
    "class FusionAtTrackingWithDetections(_AgentModel):\n",
    "    \"\"\"Treat detections from other agents as detections\"\"\" \n",
    "    def __init__(self, perception, n_agents, *args, **kwargs):\n",
    "        tracking = avstack.modules.tracking.tracker3d.BasicBoxTracker3D()\n",
    "        super().__init__(perception, tracking)\n",
    "\n",
    "    def ingest(self, ego_pc, agents_pcs):\n",
    "        dets_ego = self.perception['ego']['lidar'](ego_pc)\n",
    "        dets_all = [self.perception['agents']['lidar'](agents_pcs[i]) for i in range(len(agents_pcs))]\n",
    "        dets_all.append(dets_ego)\n",
    "        for dets in random.shuffle(dets_all):  # shuffle to avoid bias\n",
    "            tracks_out = self.tracking(dets)  # TODO: need to adjust the configuration of the tracker?\n",
    "        return tracks_out\n",
    "\n",
    "\n",
    "class FusionAtTrackingWithTracks(_AgentModel):\n",
    "    \"\"\"Treat tracks from other agents as detections\"\"\" \n",
    "    def __init__(self, perception, n_agents, *args, **kwargs):\n",
    "        tracking = {\n",
    "            'ego':avstack.modules.tracking.tracker3d.BasicBoxTracker3D(),\n",
    "            'agents':[avstack.modules.tracking.tracker3d.BasicBoxTracker3D() for _ in range(n_agents)]\n",
    "        }\n",
    "        super().__init__(perception, tracking)\n",
    "\n",
    "    def ingest(self, ego_pc, agents_pcs):\n",
    "        dets_ego = self.perception['ego'](ego_pc)\n",
    "        pseudo_dets_all = [self.tracking['agents'][i](self.perception['agents'](agents_pcs[i])) for i in range(len(agents_pcs))]\n",
    "        pseudo_dets_all.append(dets_ego)\n",
    "        for pseudo_dets in random.shuffle(pseudo_dets_all):  # shuffle to avoid bias\n",
    "            tracks_out = self.tracking(pseudo_dets)  # TODO: need to adjust the configuration of the tracker?\n",
    "        return tracks_out\n",
    "\n",
    "\n",
    "class DedicatedFusionLiDAR(_AgentModel):\n",
    "    \"\"\"Performs distributed data fusion on tracks from other agents\"\"\"\n",
    "    def __init__(self, perception, n_agents, *args, **kwargs):\n",
    "        tracking = {\n",
    "            'ego':avstack.modules.tracking.tracker3d.BasicBoxTracker3D(),\n",
    "            'agents':[avstack.modules.tracking.tracker3d.BasicBoxTracker3D() for _ in range(n_agents)]\n",
    "        }\n",
    "        super().__init__(perception, tracking)\n",
    "\n",
    "    def fusion(self, tracks_ego, tracks_agents):\n",
    "        pass\n",
    "    \n",
    "    def ingest(self, ego_pc, agents_pcs):\n",
    "        tracks_ego = self.tracking['ego'](self.perception['ego'](ego_pc))\n",
    "        tracks_agents = [self.tracking['agents'][i](self.perception['agents'](agents_pcs[i])) for i in range(len(agents_pcs))]\n",
    "        tracks_out = self.fusion(tracks_ego, tracks_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fb06f-c378-4f44-99cb-c1e2fbd00b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception = {\n",
    "    'ego':{\n",
    "        'camera':avstack.modules.perception.object2dfv.MMDetObjectDetector2D(model='fasterrcnn', dataset='carla', epoch='latest', threshold=0.5),\n",
    "        'lidar':avstack.modules.perception.object3d.MMDetObjectDetector3D(model='pointpillars', dataset='carla', epoch='latest', threshold=0.2)\n",
    "    },\n",
    "    'agent':{\n",
    "        'camera':avstack.modules.perception.object2dfv.MMDetObjectDetector2D(model='fasterrcnn', dataset='carla', threshold=0.5),\n",
    "        'lidar':avstack.modules.perception.object3d.MMDetObjectDetector3D(model='pointpillars', dataset='carla', threshold=0.2)\n",
    "    }\n",
    "}\n",
    "\n",
    "perception_reversed = {\n",
    "    'ego':{\n",
    "        'camera':perception['agent']['camera'],\n",
    "        'lidar':perception['agent']['lidar']\n",
    "    },\n",
    "    'agent':{\n",
    "        'camera':perception['ego']['camera'],\n",
    "        'lidar':perception['agent']['lidar']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9c7f4-6305-421a-86fb-a21d5c5c006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string is: algorithm-train_data-test_data\n",
    "agents = [\n",
    "    ('EgoCameraPerception-ego-ego',        EgoCameraPerception(perception),             'ego',   'ego'),\n",
    "    ('AgentsCameraPerception-agent-agent', AgentsCameraPerception(perception),          'agent', 'agent'),\n",
    "    ('AgentsCameraPerception-ego-agent',   AgentsCameraPerception(perception_reversed), 'ego',   'agent'),\n",
    "    ('EgoLidarPerception-ego-ego',         EgoLidarPerception(perception),              'ego',   'ego'),\n",
    "    ('AgentsLidarPerception-agent-agent',  AgentsLidarPerception(perception),           'agent', 'agent'),\n",
    "    ('AgentsLidarPerception-ego-agent',    AgentsLidarPerception(perception_reversed),  'ego',   'agent')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f3fad-e804-44e1-b514-5ce9db889f68",
   "metadata": {},
   "source": [
    "## Test Perception Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d7275-3625-4161-9161-ea06de15f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_ego = '/data/spencer/CARLA/ego-lidar/'\n",
    "data_dir_infra = '/data/spencer/CARLA/multi-agent-v1/'\n",
    "\n",
    "SM_ego = avapi.carla.CarlaScenesManager(data_dir_ego)\n",
    "SM_infra = avapi.carla.CarlaScenesManager(data_dir_infra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256a2a2-8ce1-454a-99f5-7271e8446742",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'application_results'\n",
    "n_scenes = 2\n",
    "n_frames = 30\n",
    "\n",
    "# perform tests for each model\n",
    "results = {}\n",
    "print('Testing {} models'.format(len(agents)))\n",
    "for i_model, (model_str, model, train, test) in enumerate(agents):\n",
    "    results[model_str] = []\n",
    "    \n",
    "    # -- get test data loader\n",
    "    if test == 'ego':\n",
    "        SM = SM_ego\n",
    "        vehicle = 'ego'\n",
    "    elif test == 'agent':\n",
    "        SM = SM_infra\n",
    "        vehicle = 'agent'\n",
    "    else:\n",
    "        raise NotImplementedError(test)\n",
    "\n",
    "    # -- determine data input type\n",
    "    if 'camera' in model_str.lower():\n",
    "        data_type = 'camera'\n",
    "    else:\n",
    "        data_type = 'lidar'\n",
    "\n",
    "    # -- make save folders\n",
    "    out_folder = os.path.join(out_dir, model_str)\n",
    "    gt_dir = os.path.join(out_folder, 'ground-truth')\n",
    "    det_dir = os.path.join(out_folder, 'detection-results')\n",
    "    if os.path.exists(gt_dir):\n",
    "        shutil.rmtree(gt_dir)\n",
    "    if os.path.exists(det_dir):\n",
    "        shutil.rmtree(det_dir)\n",
    "    os.makedirs(gt_dir, exist_ok=False)\n",
    "    os.makedirs(det_dir, exist_ok=False)\n",
    "        \n",
    "    # -- execute the model over the scene data\n",
    "    n_scenes_run = min(n_scenes, len(SM.splits_scenes['val']))\n",
    "    print('Running over {} scenes for model #{}/{} - {}'.format(n_scenes_run, i_model+1, len(agents), model_str))\n",
    "    for i_scene, scene in enumerate(tqdm(SM.splits_scenes['val'][:n_scenes_run], position=0, leave=True)):\n",
    "        SD = SM.get_scene_dataset_by_name(scene)\n",
    "\n",
    "        # -- get all the frames for applicable sensors\n",
    "        if data_type == 'camera':\n",
    "            if vehicle == 'ego':\n",
    "                sensors = [sensor for sensor in SD.sensor_IDs if\n",
    "                               ('cam' in sensor.lower()) and\n",
    "                               ('depth' not in sensor.lower()) and \n",
    "                               ('semseg' not in sensor.lower()) and \n",
    "                               ('infra' not in sensor.lower())]\n",
    "            else:\n",
    "                sensors = [sensor for sensor in SD.sensor_IDs if \n",
    "                               ('cam' in sensor.lower()) and\n",
    "                               ('depth' not in sensor.lower()) and \n",
    "                               ('semseg' not in sensor.lower()) and \n",
    "                               ('infra' in sensor.lower())]\n",
    "        else:\n",
    "            if vehicle == 'ego':\n",
    "                sensors = [sensor for sensor in SD.sensor_IDs if\n",
    "                               ('lidar' in sensor.lower()) and\n",
    "                               ('depth' not in sensor.lower()) and \n",
    "                               ('semseg' not in sensor.lower()) and \n",
    "                               ('infra' not in sensor.lower())]\n",
    "            else:\n",
    "                sensors = [sensor for sensor in SD.sensor_IDs if \n",
    "                               ('lidar' in sensor.lower()) and\n",
    "                               ('depth' not in sensor.lower()) and \n",
    "                               ('semseg' not in sensor.lower()) and \n",
    "                               ('infra' in sensor.lower())]\n",
    "                \n",
    "        # -- loop sensors that apply\n",
    "        for sensor in sensors:\n",
    "            frames = SD.get_frames(sensor=sensor)\n",
    "            # -- loop frames\n",
    "            nframes_min = min(n_frames, len(frames))\n",
    "            for frame in frames[:nframes_min]:                \n",
    "                # -- get perception data\n",
    "                try:\n",
    "                    if data_type == 'camera':\n",
    "                        data = SD.get_image(frame=frame, sensor=sensor)\n",
    "                    else:\n",
    "                        data = SD.get_lidar(frame=frame, sensor=sensor)\n",
    "                    objs = SD.get_objects(frame=frame, sensor=sensor)\n",
    "                except (FileNotFoundError, KeyError):\n",
    "                    continue\n",
    "\n",
    "                # -- determine if ego or agent data\n",
    "                if vehicle == 'ego':\n",
    "                    ego_data = data\n",
    "                    agents_data = []\n",
    "                else:\n",
    "                    ego_data = None\n",
    "                    agents_data = [data]\n",
    "\n",
    "                # -- run perception model\n",
    "                if data_type == 'camera':\n",
    "                    outputs = model(ego_image=ego_data, agents_images=agents_data)\n",
    "                else:\n",
    "                    outputs = model(ego_pc=ego_data, agents_pcs=agents_data)\n",
    "\n",
    "                # -- HACK: if it's an agent model, we are just considering one sensor at a time\n",
    "                if vehicle != 'ego':\n",
    "                    outputs = outputs[0]\n",
    "                    \n",
    "                # -- aggregate results\n",
    "                truths = avstack.datastructs.DataContainer(\n",
    "                    frame=frame,\n",
    "                    timestamp=0.0,\n",
    "                    data=[obj.box.project_to_2d_bbox(data.calibration) if data_type == 'camera' else obj.box for obj in objs],\n",
    "                    source_identifier='truths',\n",
    "                )\n",
    "                res = avapi.evaluation.ResultManager(\n",
    "                    idx=frame,\n",
    "                    detections=outputs,\n",
    "                    truths=truths,\n",
    "                    metric='2D_IoU' if data_type == 'camera' else '3D_IoU',\n",
    "                    threshold=0.5 if data_type == 'camera' else 0.3,\n",
    "                    assign_algorithm='greedy',\n",
    "                )\n",
    "                results[model_str].append(res)\n",
    "\n",
    "# save last results since it takes long AF...\n",
    "out_file = 'perception_application_results.p'\n",
    "with open(out_file, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print('Just saved resutls to {}'.format(out_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c9e911-0e9e-48eb-accf-d4f6493a4bba",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ead033-e34c-49f9-8ffc-11b42074160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "if 'results' not in locals():\n",
    "    out_file = 'perception_application_results.p'\n",
    "    with open(out_file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    print('Just loaded resutls from {}'.format(out_file))\n",
    "else:\n",
    "    print('Results variable seems to already be defined...keep going!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec791e59-5d95-46b1-9bf2-7366faaf208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into two for camera and lidar\n",
    "ap_wrapped_by_sensor = {'camera':[], 'lidar':[]}\n",
    "for model_str in results:\n",
    "    ap, _, _ = avapi.evaluation.metrics.get_ap_from_results(results[model_str], by_class=True)    \n",
    "    if 'camera' in model_str.lower():    ap_wrapped_by_sensor['camera'].append((model_str, ap))\n",
    "    else:\n",
    "        ap_wrapped_by_sensor['lidar'].append((model_str, ap))\n",
    "\n",
    "# Make plots\n",
    "for sensor, ap_wrapped in ap_wrapped_by_sensor.items():\n",
    "    print('For sensor {}'.format(sensor))\n",
    "    avapi.evaluation.metrics.barplot_ap(ap_wrapped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38b525-3ac7-4245-804f-8a15960e66b1",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f9a96-0ba4-44b2-ba00-c614dc75de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42459a6b-c836-4c5b-beee-9e643b2b8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subrow_formatter = '\\tworowsubtablecenter{{{}}}{{\\tworowsubtablecenter{{{}}}{{{}}}}}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
