{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1f286-505f-46c2-9d4f-a2578bd4b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avapi\n",
    "import avstack\n",
    "import pandas\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from agents import EgoCameraPerception, AgentsCameraPerception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f69b8c-23ee-48ce-ab33-03d45249bed5",
   "metadata": {},
   "source": [
    "## Define Perception/Tracking Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fb06f-c378-4f44-99cb-c1e2fbd00b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception = {\n",
    "    'ego':{\n",
    "        'camera':avstack.modules.perception.object2dfv.MMDetObjectDetector2D(\n",
    "            model='fasterrcnn', dataset='carla', epoch='latest', threshold=0.5),\n",
    "        'lidar':avstack.modules.perception.object3d.MMDetObjectDetector3D(\n",
    "            model='pointpillars', dataset='carla', epoch='latest', threshold=0.2)\n",
    "    },\n",
    "    'agent':{\n",
    "        'camera':avstack.modules.perception.object2dfv.MMDetObjectDetector2D(\n",
    "            model='fasterrcnn', dataset='carla-infrastructure', epoch='latest', threshold=0.5),\n",
    "        'lidar':avstack.modules.perception.object3d.MMDetObjectDetector3D(\n",
    "            model='pointpillars', dataset='carla-infrastructure', epoch='latest', threshold=0.2)\n",
    "    },\n",
    "    'joint':{\n",
    "        'camera':None,\n",
    "        'lidar':None\n",
    "    }\n",
    "}\n",
    "\n",
    "perception_reversed = {\n",
    "    'ego':{\n",
    "        'camera':perception['agent']['camera'],\n",
    "        'lidar':perception['agent']['lidar']\n",
    "    },\n",
    "    'agent':{\n",
    "        'camera':perception['ego']['camera'],\n",
    "        'lidar':perception['agent']['lidar']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9c7f4-6305-421a-86fb-a21d5c5c006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string is: algorithm-train_data-test_data\n",
    "agents = [\n",
    "    ('Camera -- Train: Vehicle;  Test: Vehicle',     EgoCameraPerception(perception),             'ego',   'ego'),\n",
    "    ('Camera -- Train: Infra.;   Test: Infra',       AgentsCameraPerception(perception),          'agent', 'agent'),\n",
    "    ('Camera -- Train: Vehicle.; Test: Infra',       AgentsCameraPerception(perception_reversed), 'ego',   'agent'),\n",
    "    ('LiDAR  -- Train: Vehicle;  Test: Vehicle',     EgoLidarPerception(perception),              'ego',   'ego'),\n",
    "    ('LiDAR  -- Train: Infra.;   Test: Infra',       AgentsLidarPerception(perception),           'agent', 'agent'),\n",
    "    ('LiDAR  -- Train: Vehicle.; Test: Infra',       AgentsLidarPerception(perception_reversed),  'ego',   'agent')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7f3fad-e804-44e1-b514-5ce9db889f68",
   "metadata": {},
   "source": [
    "## Test Perception Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d7275-3625-4161-9161-ea06de15f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_ego = '/data/spencer/CARLA/ego-lidar/'\n",
    "data_dir_infra = '/data/spencer/CARLA/multi-agent-v1/'\n",
    "\n",
    "SM_ego = avapi.carla.CarlaScenesManager(data_dir_ego)\n",
    "SM_infra = avapi.carla.CarlaScenesManager(data_dir_infra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256a2a2-8ce1-454a-99f5-7271e8446742",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'application_results'\n",
    "n_scenes = 10\n",
    "n_frames = 50\n",
    "\n",
    "# perform tests for each model\n",
    "results = {}\n",
    "print('Testing {} models'.format(len(agents)))\n",
    "for i_model, (model_str, model, train, test) in enumerate(agents):\n",
    "    results[model_str] = []\n",
    "    \n",
    "    # -- get test data loader\n",
    "    if test == 'ego':\n",
    "        SM = SM_ego\n",
    "        vehicle = 'ego'\n",
    "    elif test == 'agent':\n",
    "        SM = SM_infra\n",
    "        vehicle = 'agent'\n",
    "    else:\n",
    "        raise NotImplementedError(test)\n",
    "\n",
    "    # -- determine data input type\n",
    "    if 'camera' in model_str.lower():\n",
    "        data_type = 'camera'\n",
    "    else:\n",
    "        data_type = 'lidar'\n",
    "\n",
    "    # -- make save folders\n",
    "    out_folder = os.path.join(out_dir, model_str)\n",
    "    gt_dir = os.path.join(out_folder, 'ground-truth')\n",
    "    det_dir = os.path.join(out_folder, 'detection-results')\n",
    "    if os.path.exists(gt_dir):\n",
    "        shutil.rmtree(gt_dir)\n",
    "    if os.path.exists(det_dir):\n",
    "        shutil.rmtree(det_dir)\n",
    "    os.makedirs(gt_dir, exist_ok=False)\n",
    "    os.makedirs(det_dir, exist_ok=False)\n",
    "        \n",
    "    # -- execute the model over the scene data\n",
    "    n_scenes_run = min(n_scenes, len(SM.splits_scenes['val']))\n",
    "    print('Running over {} scenes for model #{}/{} - {}'.format(n_scenes_run, i_model+1, len(agents), model_str))\n",
    "    for i_scene, scene in enumerate(tqdm(SM.splits_scenes['val'][:n_scenes_run], position=0, leave=True)):\n",
    "        SD = SM.get_scene_dataset_by_name(scene)\n",
    "\n",
    "        # -- get all the frames for applicable sensors\n",
    "        if data_type == 'camera':\n",
    "            if vehicle == 'ego':\n",
    "                sensors = [sensor for sensor in SD.sensor_IDs if\n",
    "                               ('cam' in sensor.lower()) and\n",
    "                               ('depth' not in sensor.lower()) and \n",
    "                               ('semseg' not in sensor.lower()) and \n",
    "                               ('infra' not in sensor.lower())]\n",
    "            else:\n",
    "                sensors = [sensor for sensor in SD.sensor_IDs if \n",
    "                               ('cam' in sensor.lower()) and\n",
    "                               ('depth' not in sensor.lower()) and \n",
    "                               ('semseg' not in sensor.lower()) and \n",
    "                               ('infra' in sensor.lower())]\n",
    "        else:\n",
    "            if vehicle == 'ego':\n",
    "                sensors = [sensor for sensor in SD.sensor_IDs if\n",
    "                               ('lidar' in sensor.lower()) and\n",
    "                               ('depth' not in sensor.lower()) and \n",
    "                               ('semseg' not in sensor.lower()) and \n",
    "                               ('infra' not in sensor.lower())]\n",
    "            else:\n",
    "                sensors = [sensor for sensor in SD.sensor_IDs if \n",
    "                               ('lidar' in sensor.lower()) and\n",
    "                               ('depth' not in sensor.lower()) and \n",
    "                               ('semseg' not in sensor.lower()) and \n",
    "                               ('infra' in sensor.lower())]\n",
    "                \n",
    "        # -- loop sensors that apply\n",
    "        for sensor in sensors:\n",
    "            frames = SD.get_frames(sensor=sensor)\n",
    "            # -- loop frames\n",
    "            nframes_min = min(n_frames, len(frames))\n",
    "            for frame in frames[:nframes_min]:                \n",
    "                # -- get perception data\n",
    "                try:\n",
    "                    if data_type == 'camera':\n",
    "                        data = SD.get_image(frame=frame, sensor=sensor)\n",
    "                    else:\n",
    "                        data = SD.get_lidar(frame=frame, sensor=sensor)\n",
    "                        if 'infra' in sensor.lower():\n",
    "                            data = data.transform_to_ground()\n",
    "                    objs = SD.get_objects(frame=frame, sensor=sensor)\n",
    "                except (FileNotFoundError, KeyError):\n",
    "                    continue\n",
    "\n",
    "                # -- determine if ego or agent data\n",
    "                if vehicle == 'ego':\n",
    "                    ego_data = data\n",
    "                    agents_data = []\n",
    "                else:\n",
    "                    ego_data = None\n",
    "                    agents_data = [data]\n",
    "\n",
    "                # -- run perception model\n",
    "                if data_type == 'camera':\n",
    "                    outputs = model(ego_image=ego_data, agents_images=agents_data)\n",
    "                else:\n",
    "                    outputs = model(ego_pc=ego_data, agents_pcs=agents_data)\n",
    "\n",
    "                # -- HACK: if it's an agent model, we are just considering one sensor at a time\n",
    "                if vehicle != 'ego':\n",
    "                    outputs = outputs[0]\n",
    "                    \n",
    "                # -- aggregate results\n",
    "                truths = avstack.datastructs.DataContainer(\n",
    "                    frame=frame,\n",
    "                    timestamp=0.0,\n",
    "                    data=[obj.box.project_to_2d_bbox(data.calibration) if data_type == 'camera' else obj.box for obj in objs],\n",
    "                    source_identifier='truths',\n",
    "                )\n",
    "                res = avapi.evaluation.ResultManager(\n",
    "                    idx=frame,\n",
    "                    detections=outputs,\n",
    "                    truths=truths,\n",
    "                    metric='2D_IoU' if data_type == 'camera' else '3D_IoU',\n",
    "                    threshold=0.5 if data_type == 'camera' else 0.3,\n",
    "                    assign_algorithm='greedy',\n",
    "                )\n",
    "                results[model_str].append(res)\n",
    "\n",
    "# save last results since it takes long AF...\n",
    "out_file = 'perception_application_results.p'\n",
    "with open(out_file, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print('Just saved resutls to {}'.format(out_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c9e911-0e9e-48eb-accf-d4f6493a4bba",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ead033-e34c-49f9-8ffc-11b42074160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "if 'results' not in locals():\n",
    "    print('loading perception results...')\n",
    "    out_file = 'perception_application_results.p'\n",
    "    with open(out_file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    print('Just loaded resutls from {}'.format(out_file))\n",
    "else:\n",
    "    print('Results variable seems to already be defined...keep going!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec791e59-5d95-46b1-9bf2-7366faaf208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_out = 'perception_application_figures'\n",
    "os.makedirs(figs_out, exist_ok=True)\n",
    "\n",
    "# Separate into two for camera and lidar\n",
    "ap_wrapped_by_sensor = {'camera':[], 'lidar':[]}\n",
    "for model_str in results:\n",
    "    ap, _, _ = avapi.evaluation.metrics.get_ap_from_results(results[model_str], by_class=True)    \n",
    "    if 'camera' in model_str.lower():    ap_wrapped_by_sensor['camera'].append((model_str, ap))\n",
    "    else:\n",
    "        ap_wrapped_by_sensor['lidar'].append((model_str, ap))\n",
    "\n",
    "# Make plots\n",
    "for sensor, ap_wrapped in ap_wrapped_by_sensor.items():\n",
    "    print('For sensor {}'.format(sensor))\n",
    "    fig = avapi.evaluation.metrics.barplot_ap(ap_wrapped, reverse_subbars=True)\n",
    "    fig.savefig(os.path.join(figs_out, f'barplot_ap_{sensor}.pdf'), format='pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38b525-3ac7-4245-804f-8a15960e66b1",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c78b2c-828c-4ff4-a8d0-530bc3810d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_ego = '/data/spencer/CARLA/ego-lidar/'\n",
    "data_dir_infra = '/data/spencer/CARLA/multi-agent-v1/'\n",
    "\n",
    "SM_ego = avapi.carla.CarlaScenesManager(data_dir_ego)\n",
    "SM_infra = avapi.carla.CarlaScenesManager(data_dir_infra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eadbf1-ce33-4b96-81a9-32a6c99369bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_on_sd(SM, sensor, models, idx_val=0, idx_frame=10):\n",
    "    SD = SM.get_scene_dataset_by_name(SM.splits_scenes['val'][idx_val])\n",
    "    img = SD.get_image(frame=SD.get_frames(sensor=sensor)[idx_frame], sensor=sensor)\n",
    "    for name, model in models:\n",
    "        dets = model(img)\n",
    "        print(f'Showing model {name}')\n",
    "        avapi.visualize.snapshot.show_image_with_boxes(img, dets, inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ae7d7-75d5-4f5a-b8b7-5aa10b11ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show testing on ego\n",
    "thresh = 0.7\n",
    "perception['ego']['camera'].threshold   = thresh\n",
    "perception['agent']['camera'].threshold = thresh\n",
    "\n",
    "models = [('Train: ego', perception['ego']['camera']),\n",
    "          ('Train: infra', perception['agent']['camera'])]\n",
    "show_model_on_sd(SM_ego, 'main_camera', models, idx_val=1, idx_frame=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa433cad-e6b3-47b5-8b5e-17582a18f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show testing on infra\n",
    "thresh = 0.2\n",
    "perception['ego']['camera'].threshold   = thresh\n",
    "perception['agent']['camera'].threshold = thresh\n",
    "\n",
    "models = [('Train: ego', perception['ego']['camera']),\n",
    "          ('Train: infra', perception['agent']['camera'])]\n",
    "show_model_on_sd(SM_infra, 'CAM_INFRASTRUCTURE_001', models, idx_val=0, idx_frame=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0f0bf-ae47-4d69-96dc-a5ed3f77805d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f9a96-0ba4-44b2-ba00-c614dc75de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42459a6b-c836-4c5b-beee-9e643b2b8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subrow_formatter = '\\tworowsubtablecenter{{{}}}{{\\tworowsubtablecenter{{{}}}{{{}}}}}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
