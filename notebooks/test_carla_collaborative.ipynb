{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avapi.carla import CarlaScenesManager\n",
    "\n",
    "\n",
    "# cpath = os.path.join(\"/data/shared/CARLA/multi-agent-v1/\")\n",
    "# CSM = CarlaScenesManager(cpath)\n",
    "# CDM = CSM.get_scene_dataset_by_name(CSM.splits_scenes[\"val\"][0])\n",
    "# vid_folder = \"videos\"\n",
    "\n",
    "cpath = os.path.join(\"/data/shared/CARLA/multi-agent-intersection/\")\n",
    "CSM = CarlaScenesManager(cpath)\n",
    "CDM = CSM.get_scene_dataset_by_name(CSM.splits_scenes[\"train\"][0])\n",
    "vid_folder = \"videos_intersection\"\n",
    "\n",
    "# cpath = \"../examples/sim_results\"\n",
    "print(CSM.scenes)\n",
    "print(f\"{len(CDM)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Perception/Tracking Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we need OpenCOOD. Thankfully, most libraries like pytorch are actually compatible.\n",
    "# (1) git submodule add -b carla_demo https://github.com/zqzqz/OpenCOOD.git submodules/OpenCOOD (already added in .gitmodules)\n",
    "# (2) pip install cumm spconv-cu113\n",
    "# (3) cd submodules/OpenCOOD && python opencood/utils/setup.py build_ext --inplace\n",
    "# (4) cd submodules/OpenCOOD && python setup.py develop\n",
    "# (5) Download https://drive.google.com/file/d/11pG0kf2uR9N_o_ACBi_zfd7flGCgZt40/view?usp=sharing into models/\n",
    "# Folder structure is like models/pointpillar_attentive_fusion/{config.yaml and latest.pth}\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import opencood.hypes_yaml.yaml_utils as yaml_utils\n",
    "from opencood.tools import train_utils, inference_utils\n",
    "from opencood.data_utils.datasets import build_dataset\n",
    "from opencood.utils import box_utils\n",
    "\n",
    "\n",
    "class OpencoodPerception:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fusion_method=\"early\",\n",
    "        model_name=\"pointpillar\",\n",
    "        model_path=\"/data/shared/models/opencood/\",\n",
    "    ):\n",
    "        assert model_name in [\"pixor\", \"voxelnet\", \"second\", \"pointpillar\"]\n",
    "        assert fusion_method in [\"early\", \"intermediate\", \"late\"]\n",
    "        self.name = \"{}_{}\".format(model_name, fusion_method)\n",
    "        self.devices = \"cuda:0\"\n",
    "        self.model_name = model_name\n",
    "        self.fusion_method = fusion_method\n",
    "        self.model_dir = os.path.join(\n",
    "            model_path,\n",
    "            \"{}_{}_fusion\".format(\n",
    "                self.model_name,\n",
    "                self.fusion_method\n",
    "                if self.fusion_method != \"intermediate\"\n",
    "                else \"attentive\",\n",
    "            ),\n",
    "        )\n",
    "        self.config_file = os.path.join(self.model_dir, \"config.yaml\")\n",
    "\n",
    "        hypes = yaml_utils.load_yaml(self.config_file, None)\n",
    "        self.model = train_utils.create_model(hypes)\n",
    "        self.dataset = build_dataset(hypes, visualize=False, train=False)\n",
    "        # we assume gpu is necessary\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        _, self.model = train_utils.load_saved_model(self.model_dir, self.model)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.preprocessors = {\n",
    "            \"early\": self.dataset.preprocess_from_carla,\n",
    "            \"intermediate\": self.dataset.preprocess_from_carla,\n",
    "            \"late\": self.dataset.preprocess_from_carla,\n",
    "        }\n",
    "        self.inference_processors = {\n",
    "            \"early\": inference_utils.inference_early_fusion,\n",
    "            \"intermediate\": inference_utils.inference_intermediate_fusion,\n",
    "            \"late\": inference_utils.inference_late_fusion,\n",
    "        }\n",
    "\n",
    "    def run(self, multi_vehicle_case, ego_id):\n",
    "        batch = self.preprocessors[self.fusion_method](multi_vehicle_case, ego_id)\n",
    "        batch_data = self.dataset.collate_batch_test([batch])\n",
    "        with torch.no_grad():\n",
    "            batch_data = train_utils.to_device(batch_data, self.device)\n",
    "            pred_box_tensor, pred_score, gt_box_tensor = self.inference_processors[\n",
    "                self.fusion_method\n",
    "            ](batch_data, self.model, self.dataset)\n",
    "        if pred_box_tensor is None:\n",
    "            pred_bboxes = np.array([])\n",
    "            pred_scores = np.array([])\n",
    "        else:\n",
    "            pred_bboxes = pred_box_tensor.cpu().numpy()\n",
    "            pred_bboxes = box_utils.corner_to_center(pred_bboxes, order=\"lwh\")\n",
    "            pred_bboxes[:, 2] -= 0.5 * pred_bboxes[:, 5]\n",
    "            pred_scores = pred_score.cpu().numpy()\n",
    "        return pred_bboxes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avstack.geometry import (\n",
    "    Sphere,\n",
    "    GlobalOrigin3D,\n",
    "    transform_orientation,\n",
    "    Box3D,\n",
    "    Position,\n",
    "    Attitude,\n",
    ")\n",
    "\n",
    "from avstack.modules.perception.detections import BoxDetection\n",
    "from avstack.modules.perception.object3d import MMDetObjectDetector3D\n",
    "from avstack.modules.tracking.tracker3d import BasicBoxTracker3D\n",
    "from avstack.modules.tracking.multisensor import MeasurementBasedMultiTracker\n",
    "\n",
    "from avstack.datastructs import DataContainer\n",
    "\n",
    "# init models\n",
    "agents = list(range(len(CDM.get_agents(frame=1))))\n",
    "agent_is_static = {\n",
    "    i: \"static\" in CDM.get_agent(frame=1, agent=i).obj_type for i in agents\n",
    "}\n",
    "percep_veh = MMDetObjectDetector3D(model=\"pointpillars\", dataset=\"carla-vehicle\")\n",
    "percep_inf = MMDetObjectDetector3D(model=\"pointpillars\", dataset=\"carla-infrastructure\")\n",
    "percep_col = OpencoodPerception(model_name=\"pointpillar\", fusion_method=\"intermediate\")\n",
    "trackers = {agent: BasicBoxTracker3D() for agent in agents}\n",
    "trackers[\"central\"] = MeasurementBasedMultiTracker(tracker=BasicBoxTracker3D())\n",
    "trackers[\"collab\"] = BasicBoxTracker3D()\n",
    "\n",
    "# init data structures\n",
    "dets = {}\n",
    "tracks = {}\n",
    "imgs_all = {agent: [] for agent in agents}\n",
    "pcs_all = {agent: [] for agent in agents}\n",
    "dets_all = {agent: [] for agent in agents}\n",
    "dets_all[\"collab\"] = []\n",
    "tracks_all = {agent: [] for agent in agents}\n",
    "tracks_all[\"central\"] = []\n",
    "tracks_all[\"collab\"] = []\n",
    "agent_0_frames = CDM.get_frames(sensor=\"lidar-0\", agent=0)[1:-1]\n",
    "platforms_all = {agent: [] for agent in agents}\n",
    "\n",
    "# flags for this run\n",
    "run_distributed_perception = True\n",
    "run_distributed_tracking = True\n",
    "run_centralized_tracking = True\n",
    "run_collaborative_perception = True\n",
    "run_collaborative_tracking = True\n",
    "\n",
    "# run loop\n",
    "n_frames_max = 60\n",
    "ego_agent = agents[0]\n",
    "for frame in tqdm(agent_0_frames[: min(n_frames_max, len(agent_0_frames))]):\n",
    "    found_data = False\n",
    "    fovs = {}\n",
    "    platforms = {}\n",
    "    perception_input = {}\n",
    "    for agent in agents:\n",
    "        ###############################################\n",
    "        # GET DATA\n",
    "        ###############################################\n",
    "        lidar_sensor = \"lidar-0\"\n",
    "        camera_sensor = \"camera-0\"\n",
    "        img = CDM.get_image(frame=frame, sensor=camera_sensor, agent=agent)\n",
    "        pc = CDM.get_lidar(frame=frame, sensor=lidar_sensor, agent=agent)\n",
    "        imgs_all[agent].append(img)\n",
    "        pcs_all[agent].append(pc)\n",
    "        objs = CDM.get_objects(frame=frame, sensor=lidar_sensor, agent=agent)\n",
    "        calib = CDM.get_calibration(frame=frame, sensor=lidar_sensor, agent=agent)\n",
    "        fovs[agent] = Sphere(radius=100)\n",
    "        platforms[agent] = calib.reference\n",
    "        platforms_all[agent].append(calib.reference)\n",
    "\n",
    "        ###############################################\n",
    "        # DISTRIBUTED PERCEPTION\n",
    "        ###############################################\n",
    "        found_data = True\n",
    "        if run_distributed_perception:\n",
    "            if agent_is_static[agent]:\n",
    "                dets[agent] = percep_inf(pc)\n",
    "            else:\n",
    "                dets[agent] = percep_veh(pc)\n",
    "            dets_all[agent].append(dets[agent])\n",
    "\n",
    "        ###############################################\n",
    "        # DISTRIBUTED TRACKING USING DISTRIBUTED PERCEP\n",
    "        ###############################################\n",
    "        if run_distributed_tracking:\n",
    "            assert run_distributed_perception\n",
    "            tracks[agent] = trackers[agent](dets[agent], platform=calib.reference)\n",
    "            if not isinstance(tracks[agent], DataContainer):\n",
    "                raise\n",
    "            tracks_all[agent].append([track.box3d.copy() for track in tracks[agent]])\n",
    "\n",
    "        ###############################################\n",
    "        # PREP FOR COLLABORATIVE PERCEPTION\n",
    "        ###############################################\n",
    "        # Construct the input data structure for collaborative perception model.\n",
    "        ref_global = pc.calibration.reference.integrate(start_at=GlobalOrigin3D)\n",
    "        position = ref_global.x\n",
    "        attitude = ref_global.q\n",
    "        euler = transform_orientation(attitude, \"quat\", \"euler\")\n",
    "        lidar_pose = np.asarray(\n",
    "            [\n",
    "                *position,\n",
    "                np.degrees(euler[0]),\n",
    "                np.degrees(euler[2]),\n",
    "                np.degrees(euler[1]),\n",
    "            ]\n",
    "        )\n",
    "        perception_input[agent] = {\n",
    "            \"lidar\": pc.data.x,\n",
    "            # This pose is in OPV2V format (x, y, z, roll, yaw, pitch) and the rotation are in degrees not radians.\n",
    "            \"lidar_pose\": lidar_pose,\n",
    "        }\n",
    "\n",
    "    ###############################################\n",
    "    # COLLABORATIVE PERCEPTION/TRACKING\n",
    "    ###############################################\n",
    "    if run_collaborative_perception:\n",
    "        pred_bboxes, _ = percep_col.run(perception_input, ego_agent)\n",
    "        collab_dets = []\n",
    "        for pred_bbox in pred_bboxes:\n",
    "            det = BoxDetection(\n",
    "                \"lidar-0\",\n",
    "                Box3D(\n",
    "                    position=Position(x=pred_bbox[0:3], reference=platforms[ego_agent]),\n",
    "                    # Note that Carla coordination has a reverted y axis (?). Taking a negation of yaw seems make it right.\n",
    "                    attitude=Attitude(\n",
    "                        q=transform_orientation([0, 0, -pred_bbox[6]], \"euler\", \"quat\"),\n",
    "                        reference=platforms[ego_agent],\n",
    "                    ),\n",
    "                    hwl=pred_bbox[3:6][::-1],\n",
    "                ),\n",
    "                platforms[ego_agent],\n",
    "            )\n",
    "            collab_dets.append(det)\n",
    "        collab_dets = DataContainer(\n",
    "            data=collab_dets, frame=frame, timestamp=0, source_identifier=\"collab\"\n",
    "        )\n",
    "        dets_all[\"collab\"].append(collab_dets)\n",
    "\n",
    "    if run_collaborative_tracking:\n",
    "        tracks[\"collab\"] = trackers[\"collab\"](\n",
    "            detections=collab_dets,\n",
    "            platform=platforms[ego_agent],\n",
    "        )\n",
    "        tracks_all[\"collab\"].append(tracks[\"collab\"])\n",
    "\n",
    "    ###############################################\n",
    "    # CENTRALIZED TRACKING USING DISTRIBUTED PERCEP\n",
    "    ###############################################\n",
    "    # run central tracker on all detections\n",
    "    if found_data:\n",
    "        if run_centralized_tracking:\n",
    "            tracks[\"central\"] = trackers[\"central\"](\n",
    "                detections=dets,\n",
    "                fovs=fovs,\n",
    "                platforms=platforms,\n",
    "            )\n",
    "            tracks_all[\"central\"].append(tracks[\"central\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that the union of the two is similar to the central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from avapi.visualize.snapshot import show_lidar_bev_with_boxes, show_boxes_bev\n",
    "\n",
    "# # extent = [[-70, 70], [-70, 70], [-100, 100]]\n",
    "# extent = [[-50, 200], [-200, 50], [-100, 100]]\n",
    "\n",
    "# show_boxes_bev(boxes=tracks[\"central\"])\n",
    "# show_lidar_bev_with_boxes(pc=pc, boxes=tracks[\"central\"], extent=extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_agent_movies, make_central_movie, make_collab_movie\n",
    "\n",
    "# agent-specific movie\n",
    "# if run_distributed_tracking:\n",
    "#     print(\"Making distributed agent movies\")\n",
    "#     extent = [[-70, 70], [-70, 70], [-100, 100]]\n",
    "#     for agent in agents:\n",
    "#         make_agent_movies(\n",
    "#             imgs=imgs_all[agent],\n",
    "#             pcs=pcs_all[agent],\n",
    "#             dets=dets_all[agent],\n",
    "#             tracks=tracks_all[agent],\n",
    "#             agent=agent,\n",
    "#             extent=extent,\n",
    "#             percep_movies=False,\n",
    "#             track_movies=True,\n",
    "#             img_movies=True,\n",
    "#             bev_movies=True,\n",
    "#             vid_folder=vid_folder,\n",
    "#         )\n",
    "\n",
    "\n",
    "# central tracking movie\n",
    "if run_centralized_tracking:\n",
    "    print(\"Making centralized tracking movies\")\n",
    "    extent = [[-150, 20], [-80, 40], [-100, 100]]\n",
    "    make_central_movie(\n",
    "        pcs_all,\n",
    "        tracks_all[\"central\"],\n",
    "        ego=platforms_all[ego_agent],\n",
    "        extent=extent,\n",
    "        vid_folder=vid_folder,\n",
    "        colormethod=\"channel-4\",\n",
    "    )\n",
    "\n",
    "\n",
    "# central perception movie\n",
    "if run_collaborative_tracking:\n",
    "    print(\"Making collaborative tracking movies\")\n",
    "    make_collab_movie(\n",
    "        pcs_all,\n",
    "        tracks_all[\"collab\"],\n",
    "        ego=platforms_all[ego_agent],\n",
    "        extent=extent,\n",
    "        vid_folder=vid_folder,\n",
    "        colormethod=\"channel-4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-collaborative-r597R3hC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd8f4e39ce837d7ffede198493b2bacfdb3f796137bc7162e476cd93b1a173b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
