{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avapi.carla import CarlaScenesManager\n",
    "\n",
    "\n",
    "# cpath = os.path.join(\"../data/multi-agent-v1/\")\n",
    "cpath = \"/data/shared/CARLA/multi-agent-v1\"\n",
    "# cpath = \"../examples/sim_results\"\n",
    "CSM = CarlaScenesManager(cpath)\n",
    "print(CSM.scenes)\n",
    "CDM = CSM.get_scene_dataset_by_index(0)\n",
    "print(f\"{len(CDM)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we need OpenCOOD. Thankfully, most libraries like pytorch are actually compatible.\n",
    "# (1) git submodule add -b carla_demo https://github.com/zqzqz/OpenCOOD.git submodules/OpenCOOD (already added in .gitmodules)\n",
    "# (2) pip install cumm spconv-cu113\n",
    "# (3) cd submodules/OpenCOOD && python opencood/utils/setup.py build_ext --inplace\n",
    "# (4) cd submodules/OpenCOOD && python setup.py develop\n",
    "# (5) Download https://drive.google.com/file/d/11pG0kf2uR9N_o_ACBi_zfd7flGCgZt40/view?usp=sharing into models/\n",
    "# Folder structure is like models/pointpillar_attentive_fusion/{config.yaml and latest.pth}\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import opencood.hypes_yaml.yaml_utils as yaml_utils\n",
    "from opencood.tools import train_utils, inference_utils\n",
    "from opencood.data_utils.datasets import build_dataset\n",
    "from opencood.utils import box_utils\n",
    "\n",
    "\n",
    "class OpencoodPerception:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fusion_method=\"early\",\n",
    "        model_name=\"pointpillar\",\n",
    "        model_path=\"/data/shared/models/opencood/\",\n",
    "    ):\n",
    "        assert model_name in [\"pixor\", \"voxelnet\", \"second\", \"pointpillar\"]\n",
    "        assert fusion_method in [\"early\", \"intermediate\", \"late\"]\n",
    "        self.name = \"{}_{}\".format(model_name, fusion_method)\n",
    "        self.devices = \"cuda:0\"\n",
    "        self.model_name = model_name\n",
    "        self.fusion_method = fusion_method\n",
    "        self.model_dir = os.path.join(\n",
    "            model_path,\n",
    "            \"{}_{}_fusion\".format(\n",
    "                self.model_name,\n",
    "                self.fusion_method\n",
    "                if self.fusion_method != \"intermediate\"\n",
    "                else \"attentive\",\n",
    "            ),\n",
    "        )\n",
    "        self.config_file = os.path.join(self.model_dir, \"config.yaml\")\n",
    "\n",
    "        hypes = yaml_utils.load_yaml(self.config_file, None)\n",
    "        self.model = train_utils.create_model(hypes)\n",
    "        self.dataset = build_dataset(hypes, visualize=False, train=False)\n",
    "        # we assume gpu is necessary\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        _, self.model = train_utils.load_saved_model(self.model_dir, self.model)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.preprocessors = {\n",
    "            \"early\": self.dataset.preprocess_from_carla,\n",
    "            \"intermediate\": self.dataset.preprocess_from_carla,\n",
    "            \"late\": self.dataset.preprocess_from_carla,\n",
    "        }\n",
    "        self.inference_processors = {\n",
    "            \"early\": inference_utils.inference_early_fusion,\n",
    "            \"intermediate\": inference_utils.inference_intermediate_fusion,\n",
    "            \"late\": inference_utils.inference_late_fusion,\n",
    "        }\n",
    "\n",
    "    def run(self, multi_vehicle_case, ego_id):\n",
    "        batch = self.preprocessors[self.fusion_method](multi_vehicle_case, ego_id)\n",
    "        batch_data = self.dataset.collate_batch_test([batch])\n",
    "        with torch.no_grad():\n",
    "            batch_data = train_utils.to_device(batch_data, self.device)\n",
    "            pred_box_tensor, pred_score, gt_box_tensor = self.inference_processors[\n",
    "                self.fusion_method\n",
    "            ](batch_data, self.model, self.dataset)\n",
    "        if pred_box_tensor is None:\n",
    "            pred_bboxes = np.array([])\n",
    "            pred_scores = np.array([])\n",
    "        else:\n",
    "            pred_bboxes = pred_box_tensor.cpu().numpy()\n",
    "            pred_bboxes = box_utils.corner_to_center(pred_bboxes, order=\"lwh\")\n",
    "            pred_bboxes[:, 2] -= 0.5 * pred_bboxes[:, 5]\n",
    "            pred_scores = pred_score.cpu().numpy()\n",
    "        return pred_bboxes, pred_scores\n",
    "\n",
    "\n",
    "perception = OpencoodPerception(model_name=\"pointpillar\", fusion_method=\"intermediate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix(roll, yaw, pitch):\n",
    "    R = np.array(\n",
    "        [\n",
    "            [\n",
    "                np.cos(yaw) * np.cos(pitch),\n",
    "                np.cos(yaw) * np.sin(pitch) * np.sin(roll) - np.sin(yaw) * np.cos(roll),\n",
    "                np.cos(yaw) * np.sin(pitch) * np.cos(roll) + np.sin(yaw) * np.sin(roll),\n",
    "            ],\n",
    "            [\n",
    "                np.sin(yaw) * np.cos(pitch),\n",
    "                np.sin(yaw) * np.sin(pitch) * np.sin(roll) + np.cos(yaw) * np.cos(roll),\n",
    "                np.sin(yaw) * np.sin(pitch) * np.cos(roll) - np.cos(yaw) * np.sin(roll),\n",
    "            ],\n",
    "            [\n",
    "                -np.sin(pitch),\n",
    "                np.cos(pitch) * np.sin(roll),\n",
    "                np.cos(pitch) * np.cos(roll),\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Perception/Tracking Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avstack.geometry import Sphere, Circle, Box3D, Position, Attitude\n",
    "\n",
    "from avstack.modules.perception.detections import BoxDetection\n",
    "from avstack.modules.perception.object2dfv import MMDetObjectDetector2D\n",
    "from avstack.modules.perception.object3d import (\n",
    "    Passthrough3DObjectDetector,\n",
    "    MMDetObjectDetector3D,\n",
    ")\n",
    "from avstack.modules.tracking.tracker3d import BasicBoxTracker3D\n",
    "from avstack.modules.tracking.multisensor import MeasurementBasedMultiTracker\n",
    "from avstack.geometry import GlobalOrigin3D, transform_orientation\n",
    "\n",
    "from avstack.datastructs import DataContainer\n",
    "\n",
    "# init models\n",
    "agents = [0, 1]\n",
    "percep = Passthrough3DObjectDetector()\n",
    "# percep = MMDetObjectDetector3D(model=\"pointpillars\", dataset=\"carla-joint\")\n",
    "trackers = {agent: BasicBoxTracker3D() for agent in agents}\n",
    "trackers[\"central\"] = BasicBoxTracker3D()\n",
    "\n",
    "# run loop\n",
    "dets = {}\n",
    "tracks = {}\n",
    "imgs_all = {agent: [] for agent in agents}\n",
    "pcs_all = {agent: [] for agent in agents}\n",
    "dets_all = {agent: [] for agent in agents}\n",
    "dets_all[\"central\"] = []\n",
    "tracks_all = {agent: [] for agent in agents}\n",
    "tracks_all[\"central\"] = []\n",
    "agent_0_frames = CDM.get_frames(sensor=\"lidar-0\", agent=0)[1:-1]\n",
    "# Just choose 100 frames for testing\n",
    "agent_0_frames = agent_0_frames\n",
    "\n",
    "# Use the ego vehicle as center, do collaborative perception.\n",
    "# The predicted bboxes are also in the ego vehicle's coordination.\n",
    "ego_agent = agents[0]\n",
    "\n",
    "for frame in tqdm(agent_0_frames):\n",
    "    # run perception and individual trackers\n",
    "    found_data = False\n",
    "    fovs = {}\n",
    "    platforms = {}\n",
    "    perception_input = {}\n",
    "    for agent in agents:\n",
    "        ###############################################\n",
    "        # GET DATA\n",
    "        ###############################################\n",
    "        lidar_sensor = \"lidar-0\"\n",
    "        camera_sensor = \"camera-0\"\n",
    "        img = CDM.get_image(frame=frame, sensor=camera_sensor, agent=agent)\n",
    "        pc = CDM.get_lidar(frame=frame, sensor=lidar_sensor, agent=agent)\n",
    "        imgs_all[agent].append(img)\n",
    "        pcs_all[agent].append(pc)\n",
    "        objs = CDM.get_objects(frame=frame, sensor=lidar_sensor, agent=agent)\n",
    "        calib = CDM.get_calibration(frame=frame, sensor=lidar_sensor, agent=agent)\n",
    "        fovs[agent] = Sphere(radius=100)\n",
    "        platforms[agent] = calib.reference\n",
    "\n",
    "        # Get LiDAR pose.\n",
    "        ref_global = pc.calibration.reference.integrate(start_at=GlobalOrigin3D)\n",
    "        position = ref_global.x\n",
    "        attitude = ref_global.q\n",
    "        euler = transform_orientation(attitude, \"quat\", \"euler\")\n",
    "        lidar_pose = np.asarray(\n",
    "            [\n",
    "                *position,\n",
    "                np.degrees(euler[0]),\n",
    "                np.degrees(euler[2]),\n",
    "                np.degrees(euler[1]),\n",
    "            ]\n",
    "        )\n",
    "        # lidar_pose = np.zeros(6)\n",
    "\n",
    "        # ###############################################\n",
    "        # # DISTRIBUTED PERCEPTION\n",
    "        # ###############################################\n",
    "        # found_data = True\n",
    "        # dets[agent] = percep(objs)\n",
    "        # dets_all[agent].append(dets[agent])\n",
    "\n",
    "        # ###############################################\n",
    "        # # DISTRIBUTED TRACKING USING DISTRIBUTED PERCEP\n",
    "        # ###############################################\n",
    "        # tracks[agent] = trackers[agent](dets[agent], platform=calib.reference)\n",
    "        # if not isinstance(tracks[agent], DataContainer):\n",
    "        #     raise\n",
    "        # tracks_all[agent].append([track.box3d.copy() for track in tracks[agent]])\n",
    "\n",
    "        # Construct the input data structure for collaborative perception model.\n",
    "        perception_input[agent] = {\n",
    "            \"lidar\": pc.data.x,\n",
    "            # This pose is in OPV2V format (x, y, z, roll, yaw, pitch) and the rotation are in degrees not radians.\n",
    "            \"lidar_pose\": lidar_pose,\n",
    "        }\n",
    "\n",
    "    ###############################################\n",
    "    # CENTRALIZED PERCEPTION\n",
    "    ###############################################\n",
    "    # The predicted bboxes are in the ego vehicle's coordination.\n",
    "    # Please transform boxes to global or other vehicle' coordination according to demands.\n",
    "    pred_bboxes, _ = perception.run(perception_input, ego_agent)\n",
    "\n",
    "    central_dets = []\n",
    "    for pred_bbox in pred_bboxes:\n",
    "        det = BoxDetection(\n",
    "            \"lidar-0\",\n",
    "            Box3D(\n",
    "                position=Position(x=pred_bbox[0:3], reference=platforms[ego_agent]),\n",
    "                # Note that Carla coordination has a reverted y axis (?). Taking a negation of yaw seems make it right.\n",
    "                attitude=Attitude(\n",
    "                    q=rotation_matrix(0, -pred_bbox[6], 0),\n",
    "                    reference=platforms[ego_agent],\n",
    "                ),\n",
    "                hwl=pred_bbox[3:6][::-1],\n",
    "            ),\n",
    "            platforms[ego_agent],\n",
    "        )\n",
    "        central_dets.append(det)\n",
    "\n",
    "    dets_all[\"central\"].append(central_dets)\n",
    "\n",
    "    # ###############################################\n",
    "    # # CENTRALIZED TRACKING USING DISTRIBUTED PERCEP\n",
    "    # ###############################################\n",
    "    # # run central tracker on all detections\n",
    "    # if found_data:\n",
    "    #     tracks[\"central\"] = trackers[\"central\"](\n",
    "    #         detections=dets,\n",
    "    #         fovs=fovs,\n",
    "    #         platforms=platforms,\n",
    "    #     )\n",
    "    #     tracks_all[\"central\"].append(tracks[\"central\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that the union of the two is similar to the central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from avapi.visualize.snapshot import show_lidar_bev_with_boxes, show_boxes_bev\n",
    "\n",
    "# from avstack.geometry import GlobalOrigin3D\n",
    "\n",
    "# tracks_all_this_frame = tracks[0]+tracks[1]\n",
    "# tracks_all_this_frame = [track.change_reference(GlobalOrigin3D, inplace=False) for track in tracks_all_this_frame]\n",
    "# show_boxes_bev(boxes=tracks_all_this_frame)\n",
    "# show_boxes_bev(boxes=tracks[\"central\"])\n",
    "\n",
    "# extent = [[-70, 70], [-70, 70], [-100, 100]]\n",
    "# show_lidar_bev_with_boxes(pc=pc, boxes=tracks_all_this_frame, extent=extent)\n",
    "# show_lidar_bev_with_boxes(pc=pc, boxes=tracks[\"central\"], extent=extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avapi.visualize.movie import make_movie\n",
    "\n",
    "\n",
    "vid_folder = \"videos\"\n",
    "os.makedirs(vid_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def make_movies(imgs, pcs, dets, tracks, agent, extent=None):\n",
    "\n",
    "    ###############################################\n",
    "    # CAMERA-BASED VISUALIZATION\n",
    "    ###############################################\n",
    "\n",
    "    # perception movie\n",
    "    make_movie(\n",
    "        raw_imgs=imgs,\n",
    "        raw_pcs=pcs,\n",
    "        boxes=dets,\n",
    "        name=os.path.join(vid_folder, f\"agent-{agent}-perception-img\"),\n",
    "        save=True,\n",
    "        show_in_notebook=False,\n",
    "        projection=\"img\",\n",
    "        extent=extent,\n",
    "    )\n",
    "\n",
    "    # tracking movie\n",
    "    make_movie(\n",
    "        raw_imgs=imgs,\n",
    "        raw_pcs=pcs,\n",
    "        boxes=tracks,\n",
    "        name=os.path.join(vid_folder, f\"agent-{agent}-tracking-img\"),\n",
    "        save=True,\n",
    "        show_in_notebook=False,\n",
    "        projection=\"img\",\n",
    "        extent=extent,\n",
    "    )\n",
    "\n",
    "    ###############################################\n",
    "    # BEV-BASED VISUALIZATION\n",
    "    ###############################################\n",
    "\n",
    "    # perception movie\n",
    "    make_movie(\n",
    "        raw_imgs=imgs,\n",
    "        raw_pcs=pcs,\n",
    "        boxes=dets,\n",
    "        name=os.path.join(vid_folder, f\"agent-{agent}-perception-bev\"),\n",
    "        save=True,\n",
    "        show_in_notebook=False,\n",
    "        projection=\"bev\",\n",
    "        extent=extent,\n",
    "    )\n",
    "\n",
    "    # tracking movie\n",
    "    make_movie(\n",
    "        raw_imgs=imgs,\n",
    "        raw_pcs=pcs,\n",
    "        boxes=tracks,\n",
    "        name=os.path.join(vid_folder, f\"agent-{agent}-tracking-bev\"),\n",
    "        save=True,\n",
    "        show_in_notebook=False,\n",
    "        projection=\"bev\",\n",
    "        extent=extent,\n",
    "    )\n",
    "\n",
    "\n",
    "extent = [[-70, 70], [-70, 70], [-100, 100]]\n",
    "# for agent in agents:\n",
    "#     make_movies(\n",
    "#         imgs=imgs_all[agent],\n",
    "#         pcs=pcs_all[agent],\n",
    "#         dets=dets_all[agent],\n",
    "#         tracks=tracks_all[agent],\n",
    "#         agent=agent,\n",
    "#         extent=extent,\n",
    "#     )\n",
    "\n",
    "make_movie(\n",
    "    raw_imgs=imgs_all[ego_agent],\n",
    "    raw_pcs=pcs_all[ego_agent],\n",
    "    boxes=dets_all[\"central\"],\n",
    "    name=os.path.join(vid_folder, f\"agent-{ego_agent}-perception-bev\"),\n",
    "    save=True,\n",
    "    show_in_notebook=False,\n",
    "    projection=\"bev\",\n",
    "    extent=extent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-collaborative-r597R3hC-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd8f4e39ce837d7ffede198493b2bacfdb3f796137bc7162e476cd93b1a173b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
